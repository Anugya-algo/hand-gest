{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ring_tip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 102\u001b[0m\n\u001b[0;32m     99\u001b[0m middle_of_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \n\u001b[0;32m    101\u001b[0m is_left_hand \u001b[38;5;241m=\u001b[39m thumb_x \u001b[38;5;241m<\u001b[39m index_x  \n\u001b[1;32m--> 102\u001b[0m gesture \u001b[38;5;241m=\u001b[39m \u001b[43mrecognize_direction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhand_landmarks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlandmark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_left_hand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_left_hand:\n\u001b[0;32m    106\u001b[0m     text_position \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m)  \n",
      "Cell \u001b[1;32mIn[2], line 48\u001b[0m, in \u001b[0;36mrecognize_direction\u001b[1;34m(landmarks, is_left_hand)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m>\u001b[39m wrist\u001b[38;5;241m.\u001b[39my \u001b[38;5;129;01mand\u001b[39;00m middle_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m>\u001b[39m wrist\u001b[38;5;241m.\u001b[39my:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhand_side\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ( down)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (index_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m<\u001b[39m wrist\u001b[38;5;241m.\u001b[39my) \u001b[38;5;129;01mand\u001b[39;00m (middle_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m<\u001b[39m wrist\u001b[38;5;241m.\u001b[39my) \u001b[38;5;129;01mand\u001b[39;00m (thumb_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m>\u001b[39m wrist\u001b[38;5;241m.\u001b[39my) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[43mring_tip\u001b[49m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m>\u001b[39m wrist\u001b[38;5;241m.\u001b[39my) \u001b[38;5;129;01mand\u001b[39;00m (pinky_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m>\u001b[39m wrist\u001b[38;5;241m.\u001b[39my):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Forward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (index_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m<\u001b[39m wrist\u001b[38;5;241m.\u001b[39my) \u001b[38;5;129;01mand\u001b[39;00m (middle_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m<\u001b[39m wrist\u001b[38;5;241m.\u001b[39my) \u001b[38;5;129;01mand\u001b[39;00m (ring_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m<\u001b[39m wrist\u001b[38;5;241m.\u001b[39my) \u001b[38;5;129;01mand\u001b[39;00m (thumb_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m>\u001b[39m wrist\u001b[38;5;241m.\u001b[39my) \u001b[38;5;129;01mand\u001b[39;00m (pinky_tip\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m>\u001b[39m wrist\u001b[38;5;241m.\u001b[39my):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ring_tip' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1600)  \n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 800)  \n",
    "\n",
    "\n",
    "def recognize_direction(landmarks, is_left_hand):\n",
    "    wrist = landmarks[0]          \n",
    "    index_tip = landmarks[8]      \n",
    "    thumb_tip = landmarks[4]      \n",
    "    middle_tip = landmarks[12]    \n",
    "    pinky_tip = landmarks[20]     \n",
    "\n",
    "    hand_side = \"Left\" if is_left_hand else \"Right\"\n",
    "\n",
    "    \n",
    "    if not is_left_hand:\n",
    "        if index_tip.x > wrist.x and middle_tip.x > wrist.x:\n",
    "            return f\"{hand_side} (right)\"\n",
    "        elif thumb_tip.x > wrist.x and thumb_tip.x > index_tip.x and thumb_tip.x > pinky_tip.x:\n",
    "            return f\"{hand_side} ( right)\"\n",
    "        elif wrist.x < index_tip.x and wrist.x < pinky_tip.x:\n",
    "            return f\"{hand_side} (right)\"\n",
    "    \n",
    "    \n",
    "    if is_left_hand:\n",
    "        if index_tip.x < wrist.x and middle_tip.x < wrist.x:\n",
    "            return f\"{hand_side} ( left)\"\n",
    "        elif thumb_tip.x < index_tip.x and thumb_tip.x < pinky_tip.x:\n",
    "            return f\"{hand_side} ( left)\"\n",
    "        elif wrist.x > index_tip.x and wrist.x > pinky_tip.x:\n",
    "            return f\"{hand_side} ( left)\"\n",
    "\n",
    "    \n",
    "    if thumb_tip.y < wrist.y:  \n",
    "        return f\"{hand_side} (UP)\"\n",
    "    \n",
    "    \n",
    "    if index_tip.y > wrist.y and middle_tip.y > wrist.y:\n",
    "        return f\"{hand_side} ( down)\"\n",
    "    \n",
    "    if (index_tip.y < wrist.y) and (middle_tip.y < wrist.y) and (thumb_tip.y > wrist.y) and (ring_tip.y > wrist.y) and (pinky_tip.y > wrist.y):\n",
    "        return \" Forward\"\n",
    "\n",
    "    \n",
    "    if (index_tip.y < wrist.y) and (middle_tip.y < wrist.y) and (ring_tip.y < wrist.y) and (thumb_tip.y > wrist.y) and (pinky_tip.y > wrist.y):\n",
    "        return \"Backward\"\n",
    "    \n",
    "\n",
    "    if (thumb_tip.y > wrist.y) and (index_tip.y > wrist.y) and (middle_tip.y > wrist.y) and (ring_tip.y > wrist.y) and (pinky_tip.y > wrist.y):\n",
    "        return \"Altitude Hold\"\n",
    "    \n",
    "   \n",
    "    if (thumb_tip.y < wrist.y) and (index_tip.y < wrist.y) and (middle_tip.y < wrist.y) and (ring_tip.y < wrist.y) and (pinky_tip.y < wrist.y):\n",
    "        return \"Flip\"\n",
    "    \n",
    "    if landmarks[8].z < landmarks[0].z:  # Index finger closer to the camera (forward)\n",
    "        return 'move_forward'\n",
    "\n",
    "    # Gesture for moving backward (index finger pointing away from camera)\n",
    "    if landmarks[8].z > landmarks[0].z:  # Index finger farther from the camera (backward)\n",
    "        return 'move_backward'\n",
    "\n",
    "    return f\"{hand_side} Gesture: Unknown\"\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=False,      \n",
    "    max_num_hands=2,              \n",
    "    min_detection_confidence=0.5, \n",
    "    min_tracking_confidence=0.5   \n",
    ") as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "       \n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                \n",
    "                thumb_x = hand_landmarks.landmark[4].x\n",
    "                index_x = hand_landmarks.landmark[8].x\n",
    "                middle_of_frame = 0.5  \n",
    "\n",
    "                is_left_hand = thumb_x < index_x  \n",
    "                gesture = recognize_direction(hand_landmarks.landmark, is_left_hand)\n",
    "                \n",
    "                \n",
    "                if is_left_hand:\n",
    "                    text_position = (50, 50)  \n",
    "                else:\n",
    "                    text_position = (frame.shape[1] - 400, 50)  \n",
    "\n",
    "                \n",
    "                cv2.putText(frame, gesture, text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        \n",
    "        cv2.imshow('Hand Gesture Recognition', frame)\n",
    "\n",
    "        \n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening... Please say something.\n",
      "Recognizing...\n",
      "Recognized Text: Sorry, I could not understand the audio.\n",
      "Listening... Please say something.\n",
      "Recognizing...\n",
      "Recognized Text: sorry I could not understand the audio\n",
      "Listening... Please say something.\n",
      "Recognizing...\n",
      "Recognized Text: Sorry, I could not understand the audio.\n",
      "Listening... Please say something.\n",
      "Recognizing...\n",
      "Recognized Text: can you listen\n",
      "Listening... Please say something.\n",
      "Recognizing...\n",
      "Recognized Text: Ayush can you\n",
      "Listening... Please say something.\n",
      "Recognizing...\n",
      "Recognized Text: read your name\n",
      "Listening... Please say something.\n",
      "Recognizing...\n",
      "Recognized Text: throttle\n",
      "Listening... Please say something.\n",
      "Recognizing...\n",
      "Recognized Text: flip flop\n",
      "Listening... Please say something.\n",
      "Recognizing...\n",
      "Recognized Text: till to the right tilt to the right\n",
      "Listening... Please say something.\n",
      "Recognizing...\n",
      "Recognized Text: Sorry, I could not understand the audio.\n",
      "Listening... Please say something.\n",
      "Recognizing...\n",
      "Recognized Text: Sorry, I could not understand the audio.\n",
      "Listening... Please say something.\n",
      "Recognizing...\n",
      "Recognized Text: Sorry, I could not understand the audio.\n",
      "Listening... Please say something.\n",
      "Recognizing...\n",
      "Recognized Text: Sorry, I could not understand the audio.\n",
      "Listening... Please say something.\n",
      "Recognizing...\n",
      "Recognized Text: exit\n",
      "Exiting speech recognition.\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "class SpeechRecognizer:\n",
    "    def __init__(self):\n",
    "        self.recognizer = sr.Recognizer()\n",
    "        self.microphone = sr.Microphone()\n",
    "\n",
    "    def listen_for_audio(self):\n",
    "        \n",
    "        with self.microphone as source:\n",
    "            \n",
    "            self.recognizer.adjust_for_ambient_noise(source)\n",
    "            print(\"Say, I am Listening...\")\n",
    "            audio = self.recognizer.listen(source)\n",
    "\n",
    "        return audio\n",
    "\n",
    "    def recognize_speech(self, audio):\n",
    "        \n",
    "        try:\n",
    "            print(\"Recognized command...\")\n",
    "            text = self.recognizer.recognize_google(audio)\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            return \"Sorry, I could not understand.\"\n",
    "        except sr.RequestError:\n",
    "            return \"Sorry, an issue with the speech recognition service.\"\n",
    "\n",
    "    def start_recognition(self):\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            audio = self.listen_for_audio()\n",
    "            recognized_text = self.recognize_speech(audio)\n",
    "            print(f\"Recognized Text: {recognized_text}\")\n",
    "            \n",
    "            if recognized_text.lower() == \"exit\":\n",
    "                print(\"Exiting speech recognition.\")\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    recognizer = SpeechRecognizer()\n",
    "    recognizer.start_recognition()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1600)  # Set width to 1600\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 800)  # Set height to 800  \n",
    "\n",
    "# Gesture recognition function based on distances and directions\n",
    "def recognize_direction(landmarks, is_left_hand):\n",
    "    wrist = landmarks[0]  # Wrist\n",
    "    index_tip = landmarks[8]  # Index finger tip\n",
    "    thumb_tip = landmarks[4]  # Thumb tip\n",
    "    middle_tip = landmarks[12]  # Middle finger tip\n",
    "    ring_tip = landmarks[16]  # Ring finger tip\n",
    "    pinky_tip = landmarks[20]  # Pinky tip\n",
    "\n",
    "    # Determine the side (left or right)\n",
    "    hand_side = \"Left\" if is_left_hand else \"Right\"\n",
    "\n",
    "    # Calculate distances between wrist and finger tips (just an example using Euclidean distance)\n",
    "    def calc_distance(p1, p2):\n",
    "        return ((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2 + (p1.z - p2.z) ** 2) ** 0.5\n",
    "\n",
    "    distances = {\n",
    "        'index': calc_distance(wrist, index_tip),\n",
    "        'thumb': calc_distance(wrist, thumb_tip),\n",
    "        'middle': calc_distance(wrist, middle_tip),\n",
    "        'ring': calc_distance(wrist, ring_tip),\n",
    "        'pinky': calc_distance(wrist, pinky_tip)\n",
    "    }\n",
    "\n",
    "    # Direction-based Gestures\n",
    "    # Left and Right: Based on X positions of the wrist and finger tips\n",
    "    if thumb_tip.x < wrist.x and index_tip.x < wrist.x:  # Hand moves left\n",
    "        return \"Move Left\"\n",
    "    elif thumb_tip.x > wrist.x and index_tip.x > wrist.x:  # Hand moves right\n",
    "        return \"Move Right\"\n",
    "    \n",
    "    # Up and Down: Based on Y positions of the wrist and finger tips\n",
    "    if index_tip.y < wrist.y and middle_tip.y < wrist.y:  # Hand moves up\n",
    "        return \"Move Up\"\n",
    "    elif index_tip.y > wrist.y and middle_tip.y > wrist.y:  # Hand moves down\n",
    "        return \"Move Down\"\n",
    "    \n",
    "    # Recognizing other gestures based on distances\n",
    "    if all(dist < 0.1 for dist in distances.values()):  # All fingers near wrist (Fist)\n",
    "        return \"Altitude Hold\"\n",
    "\n",
    "    if distances['thumb'] < 0.2 and distances['index'] > 0.3:  # Thumb near wrist, index far away (Move Forward)\n",
    "        return \"Move Forward\"\n",
    "\n",
    "    if distances['thumb'] < 0.2 and distances['index'] < 0.2 and distances['middle'] < 0.2:  # 3 fingers near wrist (Move Backward)\n",
    "        return \"Move Backward\"\n",
    "\n",
    "    if distances['index'] > 0.3 and distances['middle'] > 0.3:  # Index and middle far away (Flip)\n",
    "        return \"Flip\"\n",
    "\n",
    "    return f\"{hand_side} Gesture: Unknown\"\n",
    "\n",
    "# Start Hand Detection using MediaPipe\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=False,       # Real-time mode\n",
    "    max_num_hands=2,              # Detect up to 2 hands\n",
    "    min_detection_confidence=0.5, # Detection threshold\n",
    "    min_tracking_confidence=0.5   # Tracking threshold\n",
    ") as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Flip and process the frame\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        # Draw hand landmarks and recognize gestures\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Get landmark coordinates to recognize which hand and gesture\n",
    "                thumb_x = hand_landmarks.landmark[4].x\n",
    "                index_x = hand_landmarks.landmark[8].x\n",
    "                middle_of_frame = 0.5  # Middle of the frame (normalized)\n",
    "\n",
    "                # Determine if the hand is left or right based on thumb and index positions\n",
    "                is_left_hand = thumb_x < index_x  # Left hand if thumb is on the left of the index\n",
    "\n",
    "                gesture = recognize_direction(hand_landmarks.landmark, is_left_hand)\n",
    "                \n",
    "                # Determine position for text based on the hand's side\n",
    "                if is_left_hand:\n",
    "                    text_position = (50, 50)  # Position for Left Hand (Left side of the screen)\n",
    "                else:\n",
    "                    text_position = (frame.shape[1] - 400, 50)  # Position for Right Hand (Right side of the screen)\n",
    "\n",
    "                # Display gesture text\n",
    "                cv2.putText(frame, gesture, text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Hand Gesture Recognition', frame)\n",
    "\n",
    "        # Exit when 'Esc' key is pressed\n",
    "        if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
